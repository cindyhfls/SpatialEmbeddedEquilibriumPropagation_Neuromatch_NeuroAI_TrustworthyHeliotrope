{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+fgR8QbRjMCDpUWkRUlTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindyhfls/SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope/blob/main/equilibrium_propagation_toymodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from https://github.com/smonsays/equilibrium-propagation/tree/master \"run_energy_model_mnist.py\"\n",
        "\n",
        "**To-do:**\n",
        "*Week 1 - Make the network architecture and train basic network, decide on the questions*\n",
        "1. We first make a fake \"distance\" matrix by specifying the distance between each of the 1000x1000 pairs of units.\n",
        "2. Implement spatial normalization through energy function?\n",
        "*Week 2 - Calculating metrics to evaluate the network, each person pick a direction to test and produce a summary slide.*"
      ],
      "metadata": {
        "id": "-SFoRIRNUIM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXyy3RgDRp16",
        "outputId": "2f833656-06b8-470a-c861-1e8c1e83e117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'equilibrium-propagation' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# @title Clone Repository and Setup\n",
        "!git clone https://github.com/smonsays/equilibrium-propagation.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/equilibrium-propagation/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re7mL-FKcJaF",
        "outputId": "4b7d3ed5-c4f1-4817-f4f1-f0626a074750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/equilibrium-propagation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "\n",
        "from lib import config, data, energy, train, utils"
      ],
      "metadata": {
        "id": "EHmlbpfzcE28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install torchlens and other utilities for visualization/RSA?\n",
        "!pip install torchlens --quiet\n",
        "!pip install rsatoolbox --quiet\n",
        "\n",
        "import torchlens,rsatoolbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wBSNmhO-WmmH",
        "outputId": "e18533dd-0d90-4960-b456-03982b5f3936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/83.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.3/83.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m656.0/656.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(model, imgs, return_layers, plot='none'):\n",
        "    \"\"\"\n",
        "    Extracts features from specified layers of the model.\n",
        "\n",
        "    Inputs:\n",
        "    - model (torch.nn.Module): The model from which to extract features.\n",
        "    - imgs (torch.Tensor): Batch of input images.\n",
        "    - return_layers (list): List of layer names from which to extract features.\n",
        "    - plot (str): Option to plot the features. Default is 'none'.\n",
        "\n",
        "    Outputs:\n",
        "    - model_features (dict): A dictionary with layer names as keys and extracted features as values.\n",
        "    \"\"\"\n",
        "    model_history = tl.log_forward_pass(model, imgs, layers_to_save='all', vis_opt=plot)\n",
        "    model_features = {}\n",
        "    for layer in return_layers:\n",
        "        model_features[layer] = model_history[layer].tensor_contents.flatten(1)\n",
        "\n",
        "    return model_features"
      ],
      "metadata": {
        "id": "eGaTAdGDYziP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper functions for parsing input\n",
        "def load_default_config(energy):\n",
        "    \"\"\"\n",
        "    Load default parameter configuration from file.\n",
        "\n",
        "    Args:\n",
        "        tasks: String with the energy name\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of default parameters for the given energy\n",
        "    \"\"\"\n",
        "    if energy == \"restr_hopfield\":\n",
        "        default_config = \"etc/energy_restr_hopfield.json\"\n",
        "    elif energy == \"cond_gaussian\":\n",
        "        default_config = \"etc/energy_cond_gaussian.json\"\n",
        "    else:\n",
        "        raise ValueError(\"Energy based model \\\"{}\\\" not defined.\".format(energy))\n",
        "\n",
        "    with open(default_config) as config_json_file:\n",
        "        cfg = json.load(config_json_file)\n",
        "\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def parse_shell_args(args):\n",
        "    \"\"\"\n",
        "    Parse shell arguments for this script.\n",
        "\n",
        "    Args:\n",
        "        args: List of shell arguments\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of shell arguments\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Train an energy-based model on MNIST using Equilibrium Propagation.\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=argparse.SUPPRESS,\n",
        "                        help=\"Size of mini batches during training.\")\n",
        "    parser.add_argument(\"--c_energy\", choices=[\"cross_entropy\", \"squared_error\"],\n",
        "                        default=argparse.SUPPRESS, help=\"Supervised learning cost function.\")\n",
        "    parser.add_argument(\"--dimensions\", type=int, nargs=\"+\",\n",
        "                        default=argparse.SUPPRESS, help=\"Dimensions of the neural network.\")\n",
        "    parser.add_argument(\"--energy\", choices=[\"cond_gaussian\", \"restr_hopfield\"],\n",
        "                        default=\"cond_gaussian\", help=\"Type of energy-based model.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=argparse.SUPPRESS,\n",
        "                        help=\"Number of epochs to train.\")\n",
        "    parser.add_argument(\"--fast_ff_init\", action='store_true', default=argparse.SUPPRESS,\n",
        "                        help=\"Flag to enable fast feedforward initialization.\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=argparse.SUPPRESS,\n",
        "                        help=\"Learning rate of the optimizer.\")\n",
        "    parser.add_argument(\"--log_dir\", type=str, default=\"\",\n",
        "                        help=\"Subdirectory within ./log/ to store logs.\")\n",
        "    parser.add_argument(\"--nonlinearity\", choices=[\"leaky_relu\", \"relu\", \"sigmoid\", \"tanh\"],\n",
        "                        default=argparse.SUPPRESS, help=\"Nonlinearity between network layers.\")\n",
        "    parser.add_argument(\"--optimizer\", choices=[\"adam\", \"adagrad\", \"sgd\"],\n",
        "                        default=argparse.SUPPRESS, help=\"Optimizer used to train the model.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=argparse.SUPPRESS,\n",
        "                        help=\"Random seed for pytorch\")\n",
        "\n",
        "    return vars(parser.parse_args(args))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GEkgL8VwS8nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.argv = ['','--energy', 'restr_hopfield', '--epochs', '1']\n",
        "\n",
        "# Parse shell arguments as input configuration\n",
        "user_config = parse_shell_args(sys.argv[1:])\n",
        "\n",
        "# Load default parameter configuration from file for the specified energy-based model\n",
        "cfg = load_default_config(user_config[\"energy\"])\n",
        "\n",
        "# Overwrite default parameters with user configuration where applicable\n",
        "cfg.update(user_config)\n",
        "\n",
        "# Setup global logger and logging directory\n",
        "config.setup_logging(cfg[\"energy\"] + \"_\" + cfg[\"c_energy\"] + \"_\" + cfg[\"dataset\"],\n",
        "                      dir=cfg['log_dir'])"
      ],
      "metadata": {
        "id": "tivHcdFXTeW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main function run_energy_model_mnist\n",
        "\n",
        "\"\"\"\n",
        "Main script.\n",
        "\n",
        "Args:\n",
        "    cfg: Dictionary defining parameters of the run\n",
        "\"\"\"\n",
        "# Initialize seed if specified (might slow down the model)\n",
        "if cfg['seed'] is not None:\n",
        "    torch.manual_seed(cfg['seed'])\n",
        "\n",
        "# Create the cost function to be optimized by the model\n",
        "c_energy = utils.create_cost(cfg['c_energy'], cfg['beta'])\n",
        "\n",
        "# Create activation functions for every layer as a list\n",
        "phi = utils.create_activations(cfg['nonlinearity'], len(cfg['dimensions']))\n",
        "\n",
        "# Initialize energy based model\n",
        "if cfg[\"energy\"] == \"restr_hopfield\":\n",
        "    model = energy.RestrictedHopfield(\n",
        "        cfg['dimensions'], c_energy, cfg['batch_size'], phi).to(config.device)\n",
        "elif cfg[\"energy\"] == \"cond_gaussian\":\n",
        "    model = energy.ConditionalGaussian(\n",
        "        cfg['dimensions'], c_energy, cfg['batch_size'], phi).to(config.device)\n",
        "else:\n",
        "    raise ValueError(f'Energy based model \\\"{cfg[\"energy\"]}\\\" not defined.')\n",
        "\n",
        "# Define optimizer (may include l2 regularization via weight_decay)\n",
        "w_optimizer = utils.create_optimizer(model, cfg['optimizer'],  lr=cfg['learning_rate'])\n",
        "\n",
        "# Create torch data loaders with the MNIST data set\n",
        "mnist_train, mnist_test = data.create_mnist_loaders(cfg['batch_size'])\n",
        "\n",
        "logging.info(\"Start training with parametrization:\\n{}\".format(\n",
        "    json.dumps(cfg, indent=4, sort_keys=True)))\n",
        "\n",
        "for epoch in range(1, cfg['epochs'] + 1):\n",
        "    # Training\n",
        "    train.train(model, mnist_train, cfg['dynamics'], w_optimizer, cfg[\"fast_ff_init\"])\n",
        "\n",
        "    # Testing\n",
        "    test_acc, test_energy = train.test(model, mnist_test, cfg['dynamics'], cfg[\"fast_ff_init\"])\n",
        "\n",
        "    # Logging\n",
        "    logging.info(\n",
        "        \"epoch: {} \\t test_acc: {:.4f} \\t mean_E: {:.4f}\".format(\n",
        "            epoch, test_acc, test_energy)\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RznO9RdTReC",
        "outputId": "67753e56-9127-4d27-a55b-ac8578395381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4179334.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 133700.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1273478.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2945806.21it/s]\n",
            "[INFO  11:33:08] Start training with parametrization:\n",
            "{\n",
            "    \"batch_size\": 100,\n",
            "    \"beta\": 1,\n",
            "    \"c_energy\": \"squared_error\",\n",
            "    \"dataset\": \"mnist\",\n",
            "    \"dimensions\": [\n",
            "        784,\n",
            "        1000,\n",
            "        10\n",
            "    ],\n",
            "    \"dynamics\": {\n",
            "        \"dt\": 0.1,\n",
            "        \"n_relax\": 50,\n",
            "        \"tau\": 1,\n",
            "        \"tol\": 0\n",
            "    },\n",
            "    \"energy\": \"restr_hopfield\",\n",
            "    \"epochs\": 1,\n",
            "    \"fast_ff_init\": false,\n",
            "    \"learning_rate\": 0.001,\n",
            "    \"log_dir\": \"\",\n",
            "    \"nonlinearity\": \"sigmoid\",\n",
            "    \"optimizer\": \"adam\",\n",
            "    \"seed\": null\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO  11:33:10] 0%:\tE: 327.24\tdE -25.53\tbatch_acc 0.1100\n",
            "[INFO  11:33:44] 10%:\tE: -1988.66\tdE -21.74\tbatch_acc 0.1700\n",
            "[INFO  11:34:17] 20%:\tE: -2382.46\tdE -21.02\tbatch_acc 0.3300\n",
            "[INFO  11:34:50] 30%:\tE: -2640.65\tdE -20.05\tbatch_acc 0.3700\n",
            "[INFO  11:35:24] 40%:\tE: -2907.97\tdE -18.68\tbatch_acc 0.5000\n",
            "[INFO  11:35:59] 50%:\tE: -3175.15\tdE -18.53\tbatch_acc 0.5700\n",
            "[INFO  11:36:32] 60%:\tE: -3205.23\tdE -16.03\tbatch_acc 0.6700\n",
            "[INFO  11:37:08] 70%:\tE: -3328.60\tdE -15.76\tbatch_acc 0.6700\n",
            "[INFO  11:37:41] 80%:\tE: -3560.30\tdE -13.93\tbatch_acc 0.7100\n",
            "[INFO  11:38:15] 90%:\tE: -3867.30\tdE -12.51\tbatch_acc 0.7800\n",
            "[INFO  11:39:17] epoch: 1 \t test_acc: 0.7475 \t mean_E: -3968.1018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize model\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dooKXW1WeXN",
        "outputId": "0c1091f2-8d32-4158-9f4b-eec5f3ed399d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ConditionalGaussian(\n",
            "  (W): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1000, bias=True)\n",
            "    (1): Linear(in_features=1000, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}