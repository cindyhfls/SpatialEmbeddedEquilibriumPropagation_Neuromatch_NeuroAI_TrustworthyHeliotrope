{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindyhfls/SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope/blob/main/equilibrium_propagation_toymodel_Tu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from https://github.com/smonsays/equilibrium-propagation/tree/master \"run_energy_model_mnist.py\"\n",
        "\n",
        "**To-do:**\n",
        "\n",
        "*Week 1 - Make the network architecture and train basic network, decide on the questions*\n",
        "1. We first make a fake \"distance\" matrix by specifying the distance between each of the 1000x1000 pairs of units.\n",
        "2. Implement spatial normalization through energy function?\n",
        "\n",
        "*Week 2 - Calculating metrics to evaluate the network, each person pick a direction to test and produce a summary slide.*"
      ],
      "metadata": {
        "id": "-SFoRIRNUIM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XXyy3RgDRp16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780c9428-3ad4-44e0-a542-7425329f4a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 92 (delta 48), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (92/92), 73.98 KiB | 4.35 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ]
        }
      ],
      "source": [
        "# @title Clone Repository and Setup\n",
        "!git clone https://github.com/cindyhfls/SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope.git -b BackPropDev"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope/equilibrium-propagation-master/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re7mL-FKcJaF",
        "outputId": "7bf34caf-0230-46de-f8da-9d634b250888"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope/equilibrium-propagation-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "\n",
        "from lib import config, data, energy, train, utils"
      ],
      "metadata": {
        "id": "EHmlbpfzcE28"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper functions for parsing input\n",
        "def load_default_config(energy):\n",
        "    \"\"\"\n",
        "    Load default parameter configuration from file.\n",
        "\n",
        "    Args:\n",
        "        tasks: String with the energy name\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of default parameters for the given energy\n",
        "    \"\"\"\n",
        "    if energy == \"restr_hopfield\":\n",
        "        default_config = \"etc/energy_restr_hopfield.json\"\n",
        "    elif energy == \"cond_gaussian\":\n",
        "        default_config = \"etc/energy_cond_gaussian.json\"\n",
        "    else:\n",
        "        raise ValueError(\"Energy based model \\\"{}\\\" not defined.\".format(energy))\n",
        "\n",
        "    with open(default_config) as config_json_file:\n",
        "        cfg = json.load(config_json_file)\n",
        "\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def parse_shell_args(args):\n",
        "    \"\"\"\n",
        "    Parse shell arguments for this script.\n",
        "\n",
        "    Args:\n",
        "        args: List of shell arguments\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of shell arguments\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Train an energy-based model on MNIST using Equilibrium Propagation.\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=argparse.SUPPRESS,\n",
        "                        help=\"Size of mini batches during training.\")\n",
        "    parser.add_argument(\"--c_energy\", choices=[\"cross_entropy\", \"squared_error\"],\n",
        "                        default=argparse.SUPPRESS, help=\"Supervised learning cost function.\")\n",
        "    parser.add_argument(\"--dimensions\", type=int, nargs=\"+\",\n",
        "                        default=argparse.SUPPRESS, help=\"Dimensions of the neural network.\")\n",
        "    parser.add_argument(\"--energy\", choices=[\"cond_gaussian\", \"restr_hopfield\"],\n",
        "                        default=\"cond_gaussian\", help=\"Type of energy-based model.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=argparse.SUPPRESS,\n",
        "                        help=\"Number of epochs to train.\")\n",
        "    parser.add_argument(\"--fast_ff_init\", action='store_true', default=argparse.SUPPRESS,\n",
        "                        help=\"Flag to enable fast feedforward initialization.\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=argparse.SUPPRESS,\n",
        "                        help=\"Learning rate of the optimizer.\")\n",
        "    parser.add_argument(\"--log_dir\", type=str, default=\"\",\n",
        "                        help=\"Subdirectory within ./log/ to store logs.\")\n",
        "    parser.add_argument(\"--nonlinearity\", choices=[\"leaky_relu\", \"relu\", \"sigmoid\", \"tanh\"],\n",
        "                        default=argparse.SUPPRESS, help=\"Nonlinearity between network layers.\")\n",
        "    parser.add_argument(\"--optimizer\", choices=[\"adam\", \"adagrad\", \"sgd\"],\n",
        "                        default=argparse.SUPPRESS, help=\"Optimizer used to train the model.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=argparse.SUPPRESS,\n",
        "                        help=\"Random seed for pytorch\")\n",
        "\n",
        "    return vars(parser.parse_args(args))"
      ],
      "metadata": {
        "id": "GEkgL8VwS8nx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change this input to make configuration\n",
        "\n",
        "sys.argv = ['','--energy', 'restr_hopfield', '--epochs', '1']\n",
        "\n",
        "# Parse shell arguments as input configuration\n",
        "user_config = parse_shell_args(sys.argv[1:])\n",
        "\n",
        "# Load default parameter configuration from file for the specified energy-based model\n",
        "cfg = load_default_config(user_config[\"energy\"])\n",
        "\n",
        "# Overwrite default parameters with user configuration where applicable\n",
        "cfg.update(user_config)\n",
        "\n",
        "# Setup global logger and logging directory\n",
        "config.setup_logging(cfg[\"energy\"] + \"_\" + cfg[\"c_energy\"] + \"_\" + cfg[\"dataset\"],\n",
        "                      dir=cfg['log_dir'])"
      ],
      "metadata": {
        "id": "tivHcdFXTeW4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GoJCWeQftXc",
        "outputId": "85683383-cb9d-4495-de5b-e33f2a951810"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 100,\n",
              " 'beta': 1,\n",
              " 'c_energy': 'squared_error',\n",
              " 'dataset': 'mnist',\n",
              " 'dimensions': [784, 1000, 10],\n",
              " 'dynamics': {'dt': 0.1, 'n_relax': 50, 'tau': 1, 'tol': 0},\n",
              " 'energy': 'restr_hopfield',\n",
              " 'epochs': 1,\n",
              " 'fast_ff_init': False,\n",
              " 'learning_rate': 0.001,\n",
              " 'nonlinearity': 'sigmoid',\n",
              " 'optimizer': 'adam',\n",
              " 'seed': None,\n",
              " 'log_dir': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load data\n",
        "\n",
        "# Create torch data loaders with the MNIST data set\n",
        "data_train, data_test = data.create_mnist_loaders(cfg['batch_size'])\n"
      ],
      "metadata": {
        "id": "T2ueIP6d82Id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c7e1f4-5ec4-4bd3-ded2-a6ec2cac3f33"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 17335268.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 473698.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4352455.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4671537.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main function run_energy_model_mnist\n",
        "\n",
        "\"\"\"\n",
        "Main script.\n",
        "\n",
        "Args:\n",
        "    cfg: Dictionary defining parameters of the run\n",
        "\"\"\"\n",
        "\n",
        "# Initialize seed if specified (might slow down the model)\n",
        "if cfg['seed'] is not None:\n",
        "    torch.manual_seed(cfg['seed'])\n",
        "\n",
        "# Create the cost function to be optimized by the model\n",
        "c_energy = utils.create_cost(cfg['c_energy'], cfg['beta'])\n",
        "\n",
        "# Create activation functions for every layer as a list\n",
        "phi = utils.create_activations(cfg['nonlinearity'], len(cfg['dimensions']))\n",
        "\n",
        "# Initialize energy based model\n",
        "if cfg[\"energy\"] == \"restr_hopfield\":\n",
        "    model = energy.RestrictedHopfield(\n",
        "        cfg['dimensions'], c_energy, cfg['batch_size'], phi).to(config.device)\n",
        "elif cfg[\"energy\"] == \"cond_gaussian\":\n",
        "    model = energy.ConditionalGaussian(\n",
        "        cfg['dimensions'], c_energy, cfg['batch_size'], phi).to(config.device)\n",
        "else:\n",
        "    raise ValueError(f'Energy based model \\\"{cfg[\"energy\"]}\\\" not defined.')\n",
        "\n",
        "# Define optimizer (may include l2 regularization via weight_decay)\n",
        "w_optimizer = utils.create_optimizer(model, cfg['optimizer'],  lr=cfg['learning_rate'])\n",
        "\n",
        "logging.info(\"Start training with parametrization:\\n{}\".format(\n",
        "    json.dumps(cfg, indent=4, sort_keys=True)))\n",
        "\n",
        "for epoch in range(1, cfg['epochs'] + 1):\n",
        "    # Training\n",
        "    train.train(model, data_train, cfg['dynamics'], w_optimizer, cfg[\"fast_ff_init\"])\n",
        "\n",
        "    # Testing\n",
        "    test_acc, test_energy = train.test(model, data_test, cfg['dynamics'], cfg[\"fast_ff_init\"])\n",
        "\n",
        "    # Logging\n",
        "    logging.info(\n",
        "        \"epoch: {} \\t test_acc: {:.4f} \\t mean_E: {:.4f}\".format(\n",
        "            epoch, test_acc, test_energy)\n",
        "    )\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "-RznO9RdTReC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254609f4-9de1-43fe-b983-8e4792b2598b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO  06:42:55] Start training with parametrization:\n",
            "{\n",
            "    \"batch_size\": 100,\n",
            "    \"beta\": 1,\n",
            "    \"c_energy\": \"squared_error\",\n",
            "    \"dataset\": \"mnist\",\n",
            "    \"dimensions\": [\n",
            "        784,\n",
            "        1000,\n",
            "        10\n",
            "    ],\n",
            "    \"dynamics\": {\n",
            "        \"dt\": 0.1,\n",
            "        \"n_relax\": 50,\n",
            "        \"tau\": 1,\n",
            "        \"tol\": 0\n",
            "    },\n",
            "    \"energy\": \"restr_hopfield\",\n",
            "    \"epochs\": 1,\n",
            "    \"fast_ff_init\": false,\n",
            "    \"learning_rate\": 0.001,\n",
            "    \"log_dir\": \"\",\n",
            "    \"nonlinearity\": \"sigmoid\",\n",
            "    \"optimizer\": \"adam\",\n",
            "    \"seed\": null\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main function run_backprop_model_mnist (reuse the cfg before for hyperparameters, model architecture etc.)\n",
        "# Create activation functions for every layer as a list\n",
        "phi = utils.create_activations(cfg['nonlinearity'], len(cfg['dimensions']))\n",
        "\n",
        "if cfg['c_energy'] = 'squared_error':\n",
        "  criterion = torch.nn.functional.mse_loss\n",
        "elif cfg['c_energy'] = 'cross_entropy':\n",
        "  criterion = torch.nn.functional.cross_entropy # it's classification so we use crossentropy\n",
        "else:\n",
        "  raise ValueError(\"c_energy \\\"{}\\\" not defined.\".format(cfg['c_energy']))\n",
        "\n",
        "model = energy.MLP(\n",
        "    cfg['dimensions'], cfg['batch_size'],phi).to(config.device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Define optimizer (may include l2 regularization via weight_decay)\n",
        "w_optimizer = utils.create_optimizer(model, cfg['optimizer'],  lr=cfg['learning_rate'])\n",
        "\n",
        "logging.info(\"Start training with parametrization:\\n{}\".format(\n",
        "    json.dumps(cfg, indent=4, sort_keys=True)))\n",
        "\n",
        "for epoch in range(1, cfg['epochs'] + 1):\n",
        "    # Training\n",
        "    train.train_backprop(model, data_train, criterion, w_optimizer)\n",
        "\n",
        "    # Testing\n",
        "    test_acc, test_energy = train.test_backprop(model, data_test,criterion)\n",
        "\n",
        "    # Logging\n",
        "    logging.info(\n",
        "        \"epoch: {} \\t test_acc: {:.4f} \".format(\n",
        "            epoch, test_acc)\n",
        "    )"
      ],
      "metadata": {
        "id": "IRHbjOHc8GHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8775d3d0-4f11-48b1-b2d8-29da056d532b"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO  08:11:25] Start training with parametrization:\n",
            "{\n",
            "    \"batch_size\": 100,\n",
            "    \"beta\": 1,\n",
            "    \"c_energy\": \"squared_error\",\n",
            "    \"dataset\": \"mnist\",\n",
            "    \"dimensions\": [\n",
            "        784,\n",
            "        1000,\n",
            "        10\n",
            "    ],\n",
            "    \"dynamics\": {\n",
            "        \"dt\": 0.1,\n",
            "        \"n_relax\": 50,\n",
            "        \"tau\": 1,\n",
            "        \"tol\": 0\n",
            "    },\n",
            "    \"energy\": \"restr_hopfield\",\n",
            "    \"epochs\": 1,\n",
            "    \"fast_ff_init\": false,\n",
            "    \"learning_rate\": 0.001,\n",
            "    \"log_dir\": \"\",\n",
            "    \"nonlinearity\": \"sigmoid\",\n",
            "    \"optimizer\": \"adam\",\n",
            "    \"seed\": null\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1000, bias=True)\n",
            "    (1): Linear(in_features=1000, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO  08:11:26] [0/60000 (0%)]\tLoss: 0.252224\n",
            "[INFO  08:11:28] [6000/60000 (10%)]\tLoss: 0.031685\n",
            "[INFO  08:11:29] [12000/60000 (20%)]\tLoss: 0.026064\n",
            "[INFO  08:11:31] [18000/60000 (30%)]\tLoss: 0.020284\n",
            "[INFO  08:11:33] [24000/60000 (40%)]\tLoss: 0.029132\n",
            "[INFO  08:11:34] [30000/60000 (50%)]\tLoss: 0.019507\n",
            "[INFO  08:11:36] [36000/60000 (60%)]\tLoss: 0.025215\n",
            "[INFO  08:11:38] [42000/60000 (70%)]\tLoss: 0.024002\n",
            "[INFO  08:11:40] [48000/60000 (80%)]\tLoss: 0.023578\n",
            "[INFO  08:11:42] [54000/60000 (90%)]\tLoss: 0.019355\n",
            "[INFO  08:11:44] Epoch Finished: Avg. Loss: 0.0276, Accuracy: 82.11%\n",
            "[INFO  08:11:46] Test Set: Avg. Loss: 0.0181, Accuracy: 90.61%\n",
            "[INFO  08:11:46] epoch: 1 \t test_acc: 90.6100 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del sys.modules['lib.train']\n",
        "from lib import train,energy"
      ],
      "metadata": {
        "id": "wG6ZQnmqjas5"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize model\n",
        "print(model)\n",
        "import graphviz\n",
        "\n",
        "def visualize_hopfield_structure(model):\n",
        "    dot = graphviz.Digraph()\n",
        "    layers = len(model.W)  # Assuming model.W contains weights between layers\n",
        "\n",
        "    # Add nodes for each layer\n",
        "    for i in range(layers + 1):  # +1 because there are n+1 layers if there are n sets of weights\n",
        "        dot.node(f'Layer {i}', f'Layer {i}')\n",
        "\n",
        "    # Add edges between nodes\n",
        "    for i in range(layers):\n",
        "        dot.edge(f'Layer {i}', f'Layer {i + 1}', label=f'Weights {i}')\n",
        "\n",
        "    return dot\n",
        "\n",
        "# Assuming 'model' is an instance of RestrictedHopfield\n",
        "model_dot = visualize_hopfield_structure(model)\n",
        "model_dot.render('hopfield_structure', format='png', view=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "1dooKXW1WeXN",
        "outputId": "5f5e1e23-b8da-453b-83c9-348f4b78be2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RestrictedHopfield(\n",
            "  (W): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1000, bias=True)\n",
            "    (1): Linear(in_features=1000, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hopfield_structure.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "\n",
        "def visualize_hopfield_connections(model):\n",
        "    dot = graphviz.Digraph(engine='dot', format='png')\n",
        "\n",
        "    # Assuming model has an attribute 'W' that represents the weights between layers\n",
        "    # and 'u' that might represent activations or states of each layer\n",
        "    layers = len(model.W)  # Number of weight matrices should indicate the number of connections\n",
        "    units_per_layer = [model.u[i].shape[1] for i in range(layers + 1)]  # +1 to include output layer units\n",
        "\n",
        "    # Create subgraphs for layers\n",
        "    for i in range(layers + 1):\n",
        "        with dot.subgraph(name=f'cluster_{i}') as c:\n",
        "            c.attr(label=f'Layer {i}')\n",
        "            for j in range(units_per_layer[i]):\n",
        "                c.node(f'n_{i}_{j}', f'Unit {j}')\n",
        "\n",
        "    # Connect units between layers\n",
        "    for i in range(layers):\n",
        "        for j in range(model.W[i].weight.shape[0]):  # Rows in the weight matrix for layer i\n",
        "            for k in range(model.W[i].weight.shape[1]):  # Columns in the weight matrix for layer i\n",
        "                weight = model.W[i].weight[j, k].item()  # Getting the weight from PyTorch model\n",
        "                dot.edge(f'n_{i}_{j}', f'n_{i+1}_{k}', label=f'{weight:.2f}')\n",
        "\n",
        "    return dot\n",
        "\n",
        "# Create a visualization of the model structure\n",
        "# Assuming 'model' is properly defined with weights 'W' and states 'u'\n",
        "model_dot = visualize_hopfield_connections(model)\n",
        "model_dot.render('hopfield_network', view=True)"
      ],
      "metadata": {
        "id": "tNeDTUXNuniq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "VxxEkTS77sd0",
        "outputId": "a95f71b1-00f9-4df7-9d3c-32361a1063bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1f8a688cae5d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF7LSwDWxoXz",
        "outputId": "1c351c00-821c-4854-e686-e42c5e9e9ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 100,\n",
              " 'beta': 1,\n",
              " 'c_energy': 'squared_error',\n",
              " 'dataset': 'mnist',\n",
              " 'dimensions': [784, 1000, 10],\n",
              " 'dynamics': {'dt': 0.1, 'n_relax': 50, 'tau': 1, 'tol': 0},\n",
              " 'energy': 'restr_hopfield',\n",
              " 'epochs': 1,\n",
              " 'fast_ff_init': False,\n",
              " 'learning_rate': 0.001,\n",
              " 'nonlinearity': 'sigmoid',\n",
              " 'optimizer': 'adam',\n",
              " 'seed': None,\n",
              " 'log_dir': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can't get this to work\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "writer = SummaryWriter('log/example')\n",
        "writer.add_graph(model)\n",
        "writer.close()\n",
        "\n",
        "\n",
        "\n",
        "!tensorboard --logdir=log"
      ],
      "metadata": {
        "id": "62o6LkyzeqmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(model, imgs, return_layers, plot='none'):\n",
        "    \"\"\"\n",
        "    Extracts features from specified layers of the model.\n",
        "\n",
        "    Inputs:\n",
        "    - model (torch.nn.Module): The model from which to extract features.\n",
        "    - imgs (torch.Tensor): Batch of input images.\n",
        "    - return_layers (list): List of layer names from which to extract features.\n",
        "    - plot (str): Option to plot the features. Default is 'none'.\n",
        "\n",
        "    Outputs:\n",
        "    - model_features (dict): A dictionary with layer names as keys and extracted features as values.\n",
        "    \"\"\"\n",
        "    model_history = tl.log_forward_pass(model, imgs, layers_to_save='all', vis_opt=plot)\n",
        "    model_features = {}\n",
        "    for layer in return_layers:\n",
        "        model_features[layer] = model_history[layer].tensor_contents.flatten(1)\n",
        "\n",
        "    return model_features"
      ],
      "metadata": {
        "id": "eGaTAdGDYziP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}