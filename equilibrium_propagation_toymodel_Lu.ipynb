{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindyhfls/SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope/blob/EarlyStopping/equilibrium_propagation_toymodel_Lu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adapted from https://github.com/smonsays/equilibrium-propagation/tree/master \"run_energy_model_mnist.py\"\n",
        "\n",
        "**To-do:**\n",
        "\n",
        "*Week 1 - Make the network architecture and train basic network, decide on the questions*\n",
        "1. We first make a fake \"distance\" matrix by specifying the distance between each of the 1000x1000 pairs of units.\n",
        "2. Implement spatial normalization through energy function?\n",
        "\n",
        "*Week 2 - Calculating metrics to evaluate the network, each person pick a direction to test and produce a summary slide.*"
      ],
      "metadata": {
        "id": "-SFoRIRNUIM8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXyy3RgDRp16",
        "outputId": "91183c92-9f4c-4e1f-c48a-0638b2e2a93b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope'...\n",
            "remote: Enumerating objects: 151, done.\u001b[K\n",
            "remote: Counting objects: 100% (151/151), done.\u001b[K\n",
            "remote: Compressing objects: 100% (136/136), done.\u001b[K\n",
            "remote: Total 151 (delta 78), reused 34 (delta 8), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (151/151), 88.68 KiB | 5.54 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n"
          ]
        }
      ],
      "source": [
        "# @title Clone Repository and Setup\n",
        "!git clone https://github.com/cindyhfls/SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope/equilibrium-propagation-master/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re7mL-FKcJaF",
        "outputId": "63a565c2-5b6a-4b8d-f154-b038a4b7e103"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SpatialEmbeddedEquilibriumPropagation_Neuromatch_NeuroAI_TrustworthyHeliotrope/equilibrium-propagation-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchlens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqJPieLLuwG5",
        "outputId": "95801cf5-76ba-4855-b8fa-e2d16b7a8900"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.3/83.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torchlens as tl\n",
        "\n",
        "from lib import config, data, energy, train, utils"
      ],
      "metadata": {
        "id": "EHmlbpfzcE28"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install torchlens and other utilities for visualization/RSA?\n",
        "!pip install torchlens --quiet\n",
        "!pip install rsatoolbox --quiet\n",
        "\n",
        "import torchlens,rsatoolbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wBSNmhO-WmmH",
        "outputId": "c89df13e-c2c3-4c55-c7e7-540a672b2226"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/656.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m522.2/656.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m656.0/656.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(model, imgs, return_layers, plot='none'):\n",
        "    \"\"\"\n",
        "    Extracts features from specified layers of the model.\n",
        "\n",
        "    Inputs:\n",
        "    - model (torch.nn.Module): The model from which to extract features.\n",
        "    - imgs (torch.Tensor): Batch of input images.\n",
        "    - return_layers (list): List of layer names from which to extract features.\n",
        "    - plot (str): Option to plot the features. Default is 'none'.\n",
        "\n",
        "    Outputs:\n",
        "    - model_features (dict): A dictionary with layer names as keys and extracted features as values.\n",
        "    \"\"\"\n",
        "    model_history = tl.log_forward_pass(model, imgs, layers_to_save='all', vis_opt=plot)\n",
        "    model_features = {}\n",
        "    for layer in return_layers:\n",
        "        model_features[layer] = model_history[layer].tensor_contents.flatten(1)\n",
        "\n",
        "    return model_features"
      ],
      "metadata": {
        "id": "eGaTAdGDYziP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper functions for parsing input\n",
        "def load_default_config(energy):\n",
        "    \"\"\"\n",
        "    Load default parameter configuration from file.\n",
        "\n",
        "    Args:\n",
        "        tasks: String with the energy name\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of default parameters for the given energy\n",
        "    \"\"\"\n",
        "    if energy == \"restr_hopfield\":\n",
        "        default_config = \"etc/energy_restr_hopfield.json\"\n",
        "    elif energy == \"cond_gaussian\":\n",
        "        default_config = \"etc/energy_cond_gaussian.json\"\n",
        "    else:\n",
        "        raise ValueError(\"Energy based model \\\"{}\\\" not defined.\".format(energy))\n",
        "\n",
        "    with open(default_config) as config_json_file:\n",
        "        cfg = json.load(config_json_file)\n",
        "\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def parse_shell_args(args):\n",
        "    \"\"\"\n",
        "    Parse shell arguments for this script.\n",
        "\n",
        "    Args:\n",
        "        args: List of shell arguments\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of shell arguments\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Train an energy-based model on MNIST using Equilibrium Propagation.\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=argparse.SUPPRESS,\n",
        "                        help=\"Size of mini batches during training.\")\n",
        "    parser.add_argument(\"--c_energy\", choices=[\"cross_entropy\", \"squared_error\"],\n",
        "                        default=argparse.SUPPRESS, help=\"Supervised learning cost function.\")\n",
        "    parser.add_argument(\"--dimensions\", type=int, nargs=\"+\",\n",
        "                        default=argparse.SUPPRESS, help=\"Dimensions of the neural network.\")\n",
        "    parser.add_argument(\"--energy\", choices=[\"cond_gaussian\", \"restr_hopfield\"],\n",
        "                        default=\"cond_gaussian\", help=\"Type of energy-based model.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=argparse.SUPPRESS,\n",
        "                        help=\"Number of epochs to train.\")\n",
        "    parser.add_argument(\"--fast_ff_init\", action='store_true', default=argparse.SUPPRESS,\n",
        "                        help=\"Flag to enable fast feedforward initialization.\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=argparse.SUPPRESS,\n",
        "                        help=\"Learning rate of the optimizer.\")\n",
        "    parser.add_argument(\"--log_dir\", type=str, default=\"\",\n",
        "                        help=\"Subdirectory within ./log/ to store logs.\")\n",
        "    parser.add_argument(\"--nonlinearity\", choices=[\"leaky_relu\", \"relu\", \"sigmoid\", \"tanh\"],\n",
        "                        default=argparse.SUPPRESS, help=\"Nonlinearity between network layers.\")\n",
        "    parser.add_argument(\"--optimizer\", choices=[\"adam\", \"adagrad\", \"sgd\"],\n",
        "                        default=argparse.SUPPRESS, help=\"Optimizer used to train the model.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=argparse.SUPPRESS,\n",
        "                        help=\"Random seed for pytorch\")\n",
        "\n",
        "    return vars(parser.parse_args(args))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GEkgL8VwS8nx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.argv = ['','--energy', 'restr_hopfield', '--epochs', '1']\n",
        "\n",
        "# Parse shell arguments as input configuration\n",
        "user_config = parse_shell_args(sys.argv[1:])\n",
        "\n",
        "# Load default parameter configuration from file for the specified energy-based model\n",
        "cfg = load_default_config(user_config[\"energy\"])\n",
        "\n",
        "# Overwrite default parameters with user configuration where applicable\n",
        "cfg.update(user_config)\n",
        "\n",
        "# Setup global logger and logging directory\n",
        "config.setup_logging(cfg[\"energy\"] + \"_\" + cfg[\"c_energy\"] + \"_\" + cfg[\"dataset\"],\n",
        "                      dir=cfg['log_dir'])"
      ],
      "metadata": {
        "id": "tivHcdFXTeW4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cfg['epochs'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSBLOvKr5idY",
        "outputId": "a0120817-4a96-4988-d3f9-abe8ffee40bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main function run_energy_model_mnist\n",
        "\n",
        "\"\"\"\n",
        "Main script.\n",
        "\n",
        "Args:\n",
        "    cfg: Dictionary defining parameters of the run\n",
        "\"\"\"\n",
        "\n",
        "# Initialize seed if specified (might slow down the model)\n",
        "if cfg['seed'] is not None:\n",
        "    torch.manual_seed(cfg['seed'])\n",
        "\n",
        "# Create the cost function to be optimized by the model\n",
        "c_energy = utils.create_cost(cfg['c_energy'], cfg['beta'])\n",
        "\n",
        "# Create activation functions for every layer as a list\n",
        "phi = utils.create_activations(cfg['nonlinearity'], len(cfg['dimensions']))\n",
        "\n",
        "# Initialize energy based model\n",
        "if cfg[\"energy\"] == \"restr_hopfield\":\n",
        "    model = energy.RestrictedHopfield(\n",
        "        cfg['dimensions'], c_energy, cfg['batch_size'], phi).to(config.device)\n",
        "elif cfg[\"energy\"] == \"cond_gaussian\":\n",
        "    model = energy.ConditionalGaussian(\n",
        "        cfg['dimensions'], c_energy, cfg['batch_size'], phi).to(config.device)\n",
        "else:\n",
        "    raise ValueError(f'Energy based model \\\"{cfg[\"energy\"]}\\\" not defined.')\n",
        "\n",
        "# Define optimizer (may include l2 regularization via weight_decay)\n",
        "w_optimizer = utils.create_optimizer(model, cfg['optimizer'],  lr=cfg['learning_rate'])\n",
        "\n",
        "# Create torch data loaders with the MNIST data set\n",
        "mnist_train, mnist_val, mnist_test = data.create_mnist_loaders(cfg['batch_size'])\n",
        "\n",
        "logging.info(\"Start training with parametrization:\\n{}\".format(\n",
        "    json.dumps(cfg, indent=4, sort_keys=True)))\n",
        "\n",
        "# record the validation accuracy of each epoch for early stopping\n",
        "PATIENCE = 2\n",
        "wait = 0\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(1, cfg['epochs'] + 1):\n",
        "    # Training\n",
        "    train.train(model, mnist_train, cfg['dynamics'], w_optimizer, cfg[\"fast_ff_init\"])\n",
        "\n",
        "    # Validation\n",
        "    val_acc, val_energy = train.test(model, mnist_val, cfg['dynamics'], cfg[\"fast_ff_init\"])\n",
        "\n",
        "    # Testing\n",
        "    test_acc, test_energy = train.test(model, mnist_test, cfg['dynamics'], cfg[\"fast_ff_init\"])\n",
        "\n",
        "    # Logging\n",
        "    logging.info(\n",
        "        \"epoch: {} \\t val_acc:{:.4f} \\t test_acc: {:.4f} \\t mean_E: {:.4f}\".format(\n",
        "            epoch, val_acc, test_acc, test_energy)\n",
        "    )\n",
        "\n",
        "    # early stopping\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= PATIENCE:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break"
      ],
      "metadata": {
        "id": "-RznO9RdTReC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42ea20d-7569-4558-fdf8-78074ca758b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5101046.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 135329.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1091113.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6329079.32it/s]\n",
            "[INFO  08:39:40] Start training with parametrization:\n",
            "{\n",
            "    \"batch_size\": 100,\n",
            "    \"beta\": 1,\n",
            "    \"c_energy\": \"squared_error\",\n",
            "    \"dataset\": \"mnist\",\n",
            "    \"dimensions\": [\n",
            "        784,\n",
            "        1000,\n",
            "        10\n",
            "    ],\n",
            "    \"dynamics\": {\n",
            "        \"dt\": 0.1,\n",
            "        \"n_relax\": 50,\n",
            "        \"tau\": 1,\n",
            "        \"tol\": 0\n",
            "    },\n",
            "    \"energy\": \"restr_hopfield\",\n",
            "    \"epochs\": 1,\n",
            "    \"fast_ff_init\": false,\n",
            "    \"learning_rate\": 0.001,\n",
            "    \"log_dir\": \"\",\n",
            "    \"nonlinearity\": \"sigmoid\",\n",
            "    \"optimizer\": \"adam\",\n",
            "    \"seed\": null\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO  08:39:42] 0%:\tE: 387.23\tdE -28.25\tbatch_acc 0.1200\n",
            "[INFO  08:40:10] 10%:\tE: -1395.42\tdE -21.56\tbatch_acc 0.3700\n",
            "[INFO  08:40:37] 20%:\tE: -1895.09\tdE -21.25\tbatch_acc 0.4000\n",
            "[INFO  08:41:04] 30%:\tE: -2156.34\tdE -20.97\tbatch_acc 0.3100\n",
            "[INFO  08:41:31] 40%:\tE: -2286.48\tdE -20.32\tbatch_acc 0.3600\n",
            "[INFO  08:41:58] 50%:\tE: -2466.76\tdE -19.01\tbatch_acc 0.4400\n",
            "[INFO  08:42:25] 60%:\tE: -2660.77\tdE -17.93\tbatch_acc 0.5300\n",
            "[INFO  08:42:52] 70%:\tE: -2700.86\tdE -15.90\tbatch_acc 0.6400\n",
            "[INFO  08:43:20] 80%:\tE: -2799.47\tdE -14.55\tbatch_acc 0.6500\n",
            "[INFO  08:43:47] 90%:\tE: -2931.02\tdE -14.01\tbatch_acc 0.6800\n",
            "[INFO  08:44:54] epoch: 1 \t val_acc:0.7135 \t test_acc: 0.7266 \t mean_E: -3217.3703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main function run_backprop_model_mnist (reuse the cfg before for hyperparameters, model architecture etc.)\n",
        "\n",
        "# Initialize seed if specified (might slow down the model) - this should have run before but let's state it again\n",
        "if cfg['seed'] is not None:\n",
        "    torch.manual_seed(cfg['seed'])\n",
        "\n",
        "# Create activation functions for every layer as a list\n",
        "phi = utils.create_activations(cfg['nonlinearity'], len(cfg['dimensions']))\n",
        "\n",
        "if cfg['c_energy'] == 'squared_error':\n",
        "  criterion = torch.nn.functional.mse_loss\n",
        "elif cfg['c_energy'] == 'cross_entropy':\n",
        "  criterion = torch.nn.functional.cross_entropy # it's classification so we use crossentropy\n",
        "else:\n",
        "  raise ValueError(\"c_energy \\\"{}\\\" not defined.\".format(cfg['c_energy']))\n",
        "\n",
        "model = energy.MLP(\n",
        "    cfg['dimensions'], cfg['batch_size'],phi).to(config.device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Define optimizer (may include l2 regularization via weight_decay)\n",
        "w_optimizer = utils.create_optimizer(model, cfg['optimizer'],  lr=cfg['learning_rate'])\n",
        "\n",
        "logging.info(\"Start training with parametrization:\\n{}\".format(\n",
        "    json.dumps(cfg, indent=4, sort_keys=True)))\n",
        "\n",
        "# record the validation accuracy of each epoch for early stopping\n",
        "PATIENCE = 2\n",
        "wait = 0\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(1, cfg['epochs'] + 1):\n",
        "    # Training\n",
        "    train.train_backprop(model, mnist_train, criterion, w_optimizer)\n",
        "\n",
        "    # Validation\n",
        "    val_acc, val_energy = train.test_backprop(model, mnist_val, criterion)\n",
        "\n",
        "    # Testing\n",
        "    test_acc, test_energy = train.test_backprop(model, mnist_test, criterion)\n",
        "\n",
        "    # Logging\n",
        "    logging.info(\n",
        "        \"epoch: {} \\t val_acc: {:.4f} \\t test_acc: {:.4f} \".format(\n",
        "            epoch, val_acc, test_acc)\n",
        "    )\n",
        "\n",
        "    # early stopping\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= PATIENCE:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwuk9ysD94xv",
        "outputId": "b31e9752-ee34-46d1-97ca-4fe4ed2bb71d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO  08:48:08] Start training with parametrization:\n",
            "{\n",
            "    \"batch_size\": 100,\n",
            "    \"beta\": 1,\n",
            "    \"c_energy\": \"squared_error\",\n",
            "    \"dataset\": \"mnist\",\n",
            "    \"dimensions\": [\n",
            "        784,\n",
            "        1000,\n",
            "        10\n",
            "    ],\n",
            "    \"dynamics\": {\n",
            "        \"dt\": 0.1,\n",
            "        \"n_relax\": 50,\n",
            "        \"tau\": 1,\n",
            "        \"tol\": 0\n",
            "    },\n",
            "    \"energy\": \"restr_hopfield\",\n",
            "    \"epochs\": 1,\n",
            "    \"fast_ff_init\": false,\n",
            "    \"learning_rate\": 0.001,\n",
            "    \"log_dir\": \"\",\n",
            "    \"nonlinearity\": \"sigmoid\",\n",
            "    \"optimizer\": \"adam\",\n",
            "    \"seed\": null\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=1000, bias=True)\n",
            "    (1): Linear(in_features=1000, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO  08:48:08] [0/54000 (0%)]\tLoss: 0.266421\n",
            "[INFO  08:48:09] [5400/54000 (10%)]\tLoss: 0.030395\n",
            "[INFO  08:48:11] [10800/54000 (20%)]\tLoss: 0.022674\n",
            "[INFO  08:48:12] [16200/54000 (30%)]\tLoss: 0.016640\n",
            "[INFO  08:48:14] [21600/54000 (40%)]\tLoss: 0.026233\n",
            "[INFO  08:48:15] [27000/54000 (50%)]\tLoss: 0.014742\n",
            "[INFO  08:48:16] [32400/54000 (60%)]\tLoss: 0.027329\n",
            "[INFO  08:48:18] [37800/54000 (70%)]\tLoss: 0.019997\n",
            "[INFO  08:48:19] [43200/54000 (80%)]\tLoss: 0.024122\n",
            "[INFO  08:48:20] [48600/54000 (90%)]\tLoss: 0.025336\n",
            "[INFO  08:48:22] Epoch Finished: Avg. Loss: 0.0241, Accuracy: 86.39%\n",
            "[INFO  08:48:23] Test Set: Avg. Loss: 0.0191, Accuracy: 89.87%\n",
            "[INFO  08:48:26] Test Set: Avg. Loss: 0.0188, Accuracy: 90.24%\n",
            "[INFO  08:48:26] epoch: 1 \t val_acc: 89.8667 \t test_acc: 90.2400 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize model\n",
        "print(model)\n",
        "# can't get this to work\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "writer = SummaryWriter('log/example')\n",
        "print(model.W)\n",
        "writer.add_graph(model.W)\n",
        "writer.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1dooKXW1WeXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=log"
      ],
      "metadata": {
        "id": "62o6LkyzeqmQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}